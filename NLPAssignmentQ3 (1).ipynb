{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPAssignmentQ3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVTmbU-rUzSJ"
      },
      "source": [
        "###DL Group 166: \n",
        "###S Alagu Thiruvadi Nainar -- 2019AH04028\n",
        "###Ashutosh More -- 2019AH04080 \n",
        "###Avinash Menon -- 2019AH04057\n",
        "\n",
        "NLP Assignment  Machine Transalation Set 4\n",
        "\n",
        "Q3) Remove stop words and build Unigram, Bigram and trigram,fourgram language model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4YJTCPvSVXg",
        "outputId": "6941e7f1-e332-4df5-98a4-3e315e80cdb0"
      },
      "source": [
        "import nltk, re, pprint, string\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "string.punctuation = string.punctuation +'“'+'”'+'-'+'’'+'‘'+'—'\n",
        "string.punctuation = string.punctuation.replace('.', '')\n",
        "file = open('drive/MyDrive/corpus.txt', encoding = 'utf8').read()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRpghZGVJ0GD"
      },
      "source": [
        "#preprocess data\n",
        "file_nl_removed = \"\"\n",
        "for line in file:\n",
        "  line_nl_removed = line.replace(\"\\n\", \" \")      #removes newlines\n",
        "  file_nl_removed += line_nl_removed\n",
        "file_p = \"\".join([char for char in file_nl_removed if char not in string.punctuation])   #removes all special characters"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGStV-E4J7EI",
        "outputId": "7ce17013-5741-49af-bb23-b28c38c90619"
      },
      "source": [
        "sents = nltk.sent_tokenize(file_p)\n",
        "print(\"The number of sentences is\", len(sents)) \n",
        "#prints the number of sentences\n",
        "words = nltk.word_tokenize(file_p)\n",
        "print(\"The number of tokens is\", len(words)) \n",
        "#prints the number of tokens\n",
        "average_tokens = round(len(words)/len(sents))\n",
        "print(\"The average number of tokens per sentence is\",\n",
        "average_tokens) \n",
        "#prints the average number of tokens per sentence\n",
        "unique_tokens = set(words)\n",
        "print(\"The number of unique tokens are\", len(unique_tokens)) \n",
        "#prints the number of unique tokens"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of sentences is 981\n",
            "The number of tokens is 27361\n",
            "The average number of tokens per sentence is 28\n",
            "The number of unique tokens are 3039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0XcO_Z3goJ8",
        "outputId": "2ba0be07-dc19-43a4-836e-c21f1bac69dd"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "unigram_with_stopwords=[]\n",
        "unigram=[]\n",
        "bigram=[]\n",
        "trigram=[]\n",
        "fourgram=[]\n",
        "\n",
        "for sentence in sents:\n",
        "    sentence = sentence.lower()\n",
        "    sequence = word_tokenize(sentence)\n",
        "    for word in sequence:\n",
        "        if (word =='.'):\n",
        "            sequence.remove(word) \n",
        "        else:\n",
        "            unigram_with_stopwords.append(word)\n",
        "\n",
        "\n",
        "#unigram, bigram, trigram, and fourgram models are created\n",
        "unigram= [p for p in unigram_with_stopwords if p not in stop_words]\n",
        "bigram.extend(list(ngrams(unigram, 2)))  \n",
        "trigram.extend(list(ngrams(unigram, 3)))\n",
        "fourgram.extend(list(ngrams(unigram, 4)))\n",
        "\n",
        "\n",
        "fdist = nltk.FreqDist(unigram)\n",
        "print(\"Most common unigrams: \", fdist.most_common(10))\n",
        "\n",
        "fdist = nltk.FreqDist(bigram)\n",
        "print(\"Most common bigrams: \", fdist.most_common(10))\n",
        "\n",
        "fdist = nltk.FreqDist(trigram)\n",
        "print(\"Most common trigrams: \", fdist.most_common(10))\n",
        "\n",
        "fdist = nltk.FreqDist(fourgram)\n",
        "print(\"Most common fourgram: \", fdist.most_common(10))\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most common unigrams:  [('said', 462), ('alice', 385), ('little', 128), ('one', 101), ('like', 85), ('know', 85), ('would', 83), ('went', 83), ('could', 77), ('thought', 74)]\n",
            "Most common bigrams:  [(('said', 'alice'), 122), (('mock', 'turtle'), 54), (('march', 'hare'), 31), (('said', 'king'), 29), (('thought', 'alice'), 26), (('white', 'rabbit'), 22), (('said', 'hatter'), 22), (('said', 'mock'), 20), (('said', 'caterpillar'), 18), (('said', 'gryphon'), 18)]\n",
            "Most common trigrams:  [(('said', 'mock', 'turtle'), 20), (('said', 'march', 'hare'), 9), (('poor', 'little', 'thing'), 6), (('little', 'golden', 'key'), 5), (('certainly', 'said', 'alice'), 5), (('white', 'kid', 'gloves'), 5), (('march', 'hare', 'said'), 5), (('mock', 'turtle', 'said'), 5), (('know', 'said', 'alice'), 4), (('might', 'well', 'say'), 4)]\n",
            "Most common fourgram:  [(('join', 'dance', 'wont', 'wont'), 4), (('wont', 'wont', 'join', 'dance'), 4), (('said', 'alice', 'little', 'timidly'), 3), (('soooop', 'eeevening', 'beautiful', 'beautiful'), 3), (('eeevening', 'beautiful', 'beautiful', 'soup'), 3), (('said', 'king', 'white', 'rabbit'), 3), (('like', 'might', 'well', 'say'), 2), (('pair', 'white', 'kid', 'gloves'), 2), (('began', 'thinking', 'children', 'knew'), 2), (('good', 'deal', 'frightened', 'sudden'), 2)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}